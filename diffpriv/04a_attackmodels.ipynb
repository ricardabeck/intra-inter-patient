{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of attack models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as spio\n",
    "from os.path import join as osj\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains and tests multiple attacker models for membership and attribute inference.\n",
    "\n",
    "The best and worst performing DP setups are used here to further analyse their privacy level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Attribute Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dp_signals(m, e):\n",
    "    with open(osj(\"..\", \"data_dp\", f\"{m}_{e}.pkl\"), \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_real_data():\n",
    "    dict_samples = spio.loadmat('../data/s2s_mitbih_aami.mat')\n",
    "    samples = dict_samples['s2s_mitbih']\n",
    "    values = samples[0]['seg_values']\n",
    "    return values\n",
    "\n",
    "def get_patient_infos():\n",
    "    with open(osj(\"..\", \"data\", \"patient_infos.pkl\"), \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# def get_ids():\n",
    "#     with open(osj(\"..\", \"data\", \"all_patients.pkl\"), \"rb\") as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "def get_patient_attribute(attribute, data):\n",
    "    attr_dict = {str(pid): data[attribute] for pid, data in data.items()}\n",
    "    attr_array = list(attr_dict.values())\n",
    "    return attr_array\n",
    "\n",
    "def read_intra_attack_setups():\n",
    "    intra_setups = pd.read_csv('../results_dp/attack_setup_intra.csv')\n",
    "    return intra_setups\n",
    "\n",
    "def read_inter_attack_setups():\n",
    "    inter_setups = pd.read_csv('../results_dp/attack_setup_inter.csv')\n",
    "    return inter_setups\n",
    "\n",
    "def get_attack_setups():\n",
    "    \"\"\"\n",
    "    Loads the attack setups for intra and inter patient attacks.\n",
    "\n",
    "    Returns:\n",
    "        dict_all_setups: dict with structure {model: {epsilon: [delta]}}\n",
    "    \"\"\"\n",
    "\n",
    "    # intra\n",
    "    intra_setups = read_intra_attack_setups()\n",
    "    intra_setups = intra_setups.sort_values(by=[\"Model\", \"Epsilon\", \"Delta\"], ascending=False)\n",
    "    intra_setups[\"Model\"] = intra_setups[\"Model\"].str.replace(\"Intra-\", \"\", regex=False)\n",
    "\n",
    "    # inter\n",
    "    inter_setups = read_inter_attack_setups()\n",
    "    inter_setups = inter_setups.sort_values(by=[\"Model\", \"Epsilon\", \"Delta\"], ascending=False)\n",
    "    inter_setups[\"Model\"] = inter_setups[\"Model\"].str.replace(\"Inter-\", \"\", regex=False)\n",
    "\n",
    "    # all\n",
    "    all_setups = pd.concat([intra_setups, inter_setups], axis=0)\n",
    "    all_setups.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    all_setups = all_setups.sort_values(by=[\"Model\", \"Epsilon\", \"Delta\"], ascending=False)\n",
    "\n",
    "    # dict\n",
    "    dict_all_setups = {}\n",
    "    for _, row in all_setups.iterrows():\n",
    "        model = row[\"Model\"]\n",
    "        epsilon = row[\"Epsilon\"]\n",
    "        delta = row[\"Delta\"]\n",
    "        \n",
    "        if model not in dict_all_setups:\n",
    "            dict_all_setups[model] = {}\n",
    "        if epsilon not in dict_all_setups[model]:\n",
    "            dict_all_setups[model][epsilon] = []\n",
    "        \n",
    "        dict_all_setups[model][epsilon].append(delta)\n",
    "\n",
    "    return dict_all_setups, len(all_setups)\n",
    "\n",
    "\n",
    "def flatten_data(signals, attr_array):\n",
    "    \"\"\"\n",
    "    Flatten the signals to have all beats of a patient in one array.\n",
    "\n",
    "    Parameters:\n",
    "        signals: ndarray with structure [patient][segment][1][beat (280,)]\n",
    "        attr_array: List or ndarray, target attribute\n",
    "\n",
    "    Returns:\n",
    "        X: ndarray in format [n_patients, n_values]\n",
    "        y: ndarray in format [n_patients]\n",
    "    \"\"\"\n",
    "    # PREPARE X\n",
    "    n_beats = 1500\n",
    "    beat_length = 280\n",
    "\n",
    "    new_signal = {}\n",
    "    for patient_idx in range(0, len(signals)): # 48\n",
    "\n",
    "        all_values = []\n",
    "        for beat_idx in range(n_beats):\n",
    "            for value_idx in range(0, beat_length): #280 values per beat\n",
    "                all_values.append(signals[patient_idx][beat_idx][0][value_idx].item())\n",
    "            \n",
    "        new_signal[patient_idx] = all_values\n",
    "    \n",
    "    signal_array = np.array(list(new_signal.values()))\n",
    "    signal_array.shape\n",
    "    X = signal_array\n",
    "    \n",
    "    # PREPARE Y\n",
    "    attr_array = np.array(attr_array)\n",
    "    attr_array.shape\n",
    "    y = attr_array\n",
    "\n",
    "    return X, y   \n",
    "\n",
    "def train_inference(X, y, binary):\n",
    "    \"\"\"\n",
    "    Trains gender inference model using Random Forest Classifier.\n",
    "\n",
    "    Parameters:\n",
    "        X: ndarray with shape [n_patients, n_values]\n",
    "        y: ndarray with shape [n_patients]\n",
    "        binary: bool, used for performance metrics\n",
    "\n",
    "    Returns:\n",
    "        metrics: dict with accuracy, precision, recall, f1-score, mae, mse, r2\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    if binary == True:\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "    elif binary == False:   \n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # save the metrics\n",
    "    metrics = {\n",
    "        \"acc\": acc,\n",
    "        \"rec\": rec,\n",
    "        \"pre\": pre,\n",
    "        \"f1\": f1,\n",
    "        \"mae\": mae,\n",
    "        \"mse\": mse,\n",
    "        \"r2\": r2\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def save_attack_performance(attack, metrics):    \n",
    "    with open(osj(\"..\", \"results_attacks\", f\"{attack}_performance.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(metrics, f) \n",
    "\n",
    "def load_attack_performance(attack):    \n",
    "    with open(osj(\"..\", \"results_attacks\", f\"{attack}_performance.pkl\"), \"rb\") as f:\n",
    "        metrics = pickle.load(f) \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attack on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 17:20:35 - INFO - Skipping existing gender inference\n",
      "2025-05-14 17:20:35 - INFO - Skipping existing age inference\n"
     ]
    }
   ],
   "source": [
    "attack = \"AIA\" # Attribute Inference Attack\n",
    "attributes = [\"gender\", \"age\"]\n",
    "patient_infos = get_patient_infos()\n",
    "real_signals = get_real_data()\n",
    "\n",
    "aia_metrics = load_attack_performance(attack)\n",
    "\n",
    "mechanism = \"no_dp\"\n",
    "\n",
    "if mechanism not in aia_metrics.keys(): \n",
    "    aia_metrics[mechanism] = {}\n",
    "\n",
    "attribute_metrics = {}\n",
    "\n",
    "### ITERATION OVER ATTRIBUTES ###\n",
    "for attribute in attributes:\n",
    "\n",
    "    attr_array = get_patient_attribute(attribute, patient_infos)\n",
    "\n",
    "    if attribute in aia_metrics[mechanism].keys(): \n",
    "        logger.info(f\"Skipping existing {attribute} inference\")\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "\n",
    "        aia_metrics[mechanism][attribute] = {}\n",
    "\n",
    "        if attribute == \"gender\": \n",
    "                            \n",
    "            X, y = flatten_data(real_signals, attr_array)\n",
    "            logger.info(f\"Training {attribute} inference\")\n",
    "            \n",
    "            y = np.where(y == 'M', 0, 1)\n",
    "\n",
    "            metrics = train_inference(X, y, binary=True)\n",
    "\n",
    "        if attribute == \"age\":\n",
    "\n",
    "\n",
    "            X, y = flatten_data(real_signals,attr_array)\n",
    "            logger.info(f\"Training {attribute} inference\")\n",
    "            \n",
    "            # y has two entries with age \"-1\" which need to be removed\n",
    "            valid_patients = y >= 0\n",
    "            X = X[valid_patients]\n",
    "            y = y[valid_patients]\n",
    "            y_grouped = pd.cut(y, bins=[0, 40, 70, 100], labels=[\"0\", \"1\", \"2\"]).astype(int)\n",
    "\n",
    "            metrics = train_inference(X, y_grouped, binary=False)\n",
    "        \n",
    "        aia_metrics[mechanism][attribute] = metrics\n",
    "\n",
    "    # save the metrics\n",
    "    save_attack_performance(attack, aia_metrics)\n",
    "    logger.info(f\"Saved attack performance for {attack}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attack on private data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_setups, len_setups = get_attack_setups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0.91, 0.81, 0.71, 0.61, 0.51, 0.081, 0.071, 0.061, 0.051, 0.041, 0.031, 0.021, 0.01, 0.001])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_setups[\"laplace\"].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.91, 0.6 - (1/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.91, 0.0 - (2/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.81, 1.0 - (3/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.81, 0.0 - (4/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.71, 0.0 - (5/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.61, 0.6 - (6/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.61, 0.2 - (7/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.61, 0.0 - (8/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.51, 0.0 - (9/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.081, 1.0 - (10/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.081, 0.7 - (11/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.071, 0.7 - (12/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.071, 0.0 - (13/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.061, 0.0 - (14/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.051, 0.8 - (15/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.041, 0.9 - (16/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.041, 0.0 - (17/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.031, 0.1 - (18/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.031, 0.0 - (19/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.021, 0.1 - (20/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.021, 0.0 - (21/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.01, 0.1 - (22/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.01, 0.0 - (23/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.001, 0.1 - (24/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for laplace, 0.001, 0.0 - (25/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.81, 0.9 - (26/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.81, 0.8 - (27/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.71, 0.8 - (28/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.71, 0.5 - (29/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.71, 0.2 - (30/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.61, 0.9 - (31/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.61, 0.3 - (32/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.51, 0.8 - (33/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.41, 0.2 - (34/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.091, 0.4 - (35/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.081, 1.0 - (36/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.071, 1.0 - (37/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.051, 0.8 - (38/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.041, 0.1 - (39/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.01, 0.8 - (40/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.01, 0.5 - (41/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for gaussian_a, 0.001, 0.8 - (42/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for bounded_n, 0.91, 0.2 - (43/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for bounded_n, 0.31, 0.2 - (44/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for bounded_n, 0.21, 0.2 - (45/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for bounded_n, 0.11, 0.3 - (46/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for bounded_n, 0.11, 0.1 - (47/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for bounded_n, 0.081, 0.2 - (48/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for bounded_n, 0.071, 0.4 - (49/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for bounded_n, 0.071, 0.2 - (50/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for bounded_n, 0.021, 0.4 - (51/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for bounded_n, 0.021, 0.1 - (52/54)\n",
      "2025-05-13 22:12:08 - INFO - gender: Skipping existing inference for bounded_n, 0.01, 0.4 - (53/54)\n",
      "2025-05-13 22:12:08 - INFO - Loading bounded_n data until epsilon 0.091 (1-2 minutes) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 22:12:15 - INFO - gender: No data available for bounded_n with epsilon 0.01 and delta 0.1.\n",
      "2025-05-13 22:12:16 - INFO - Saved attack performance for AIA.\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.91, 0.6 - (1/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.91, 0.0 - (2/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.81, 1.0 - (3/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.81, 0.0 - (4/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.71, 0.0 - (5/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.61, 0.6 - (6/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.61, 0.2 - (7/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.61, 0.0 - (8/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.51, 0.0 - (9/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.081, 1.0 - (10/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.081, 0.7 - (11/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.071, 0.7 - (12/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.071, 0.0 - (13/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.061, 0.0 - (14/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.051, 0.8 - (15/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.041, 0.9 - (16/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.041, 0.0 - (17/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.031, 0.1 - (18/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.031, 0.0 - (19/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.021, 0.1 - (20/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.021, 0.0 - (21/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.01, 0.1 - (22/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.01, 0.0 - (23/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.001, 0.1 - (24/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for laplace, 0.001, 0.0 - (25/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.81, 0.9 - (26/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.81, 0.8 - (27/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.71, 0.8 - (28/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.71, 0.5 - (29/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.71, 0.2 - (30/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.61, 0.9 - (31/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.61, 0.3 - (32/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.51, 0.8 - (33/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.41, 0.2 - (34/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.091, 0.4 - (35/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.081, 1.0 - (36/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.071, 1.0 - (37/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.051, 0.8 - (38/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.041, 0.1 - (39/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.01, 0.8 - (40/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.01, 0.5 - (41/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for gaussian_a, 0.001, 0.8 - (42/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for bounded_n, 0.91, 0.2 - (43/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for bounded_n, 0.31, 0.2 - (44/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for bounded_n, 0.21, 0.2 - (45/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for bounded_n, 0.11, 0.3 - (46/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for bounded_n, 0.11, 0.1 - (47/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for bounded_n, 0.081, 0.2 - (48/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for bounded_n, 0.071, 0.4 - (49/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for bounded_n, 0.071, 0.2 - (50/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for bounded_n, 0.021, 0.4 - (51/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for bounded_n, 0.021, 0.1 - (52/54)\n",
      "2025-05-13 22:12:16 - INFO - age: Skipping existing inference for bounded_n, 0.01, 0.4 - (53/54)\n",
      "2025-05-13 22:12:16 - INFO - Loading bounded_n data until epsilon 0.091 (1-2 minutes) ...\n",
      "2025-05-13 22:12:21 - INFO - age: No data available for bounded_n with epsilon 0.01 and delta 0.1.\n",
      "2025-05-13 22:12:22 - INFO - Saved attack performance for AIA.\n"
     ]
    }
   ],
   "source": [
    "attack = \"AIA\" # Attribute Inference Attack\n",
    "attributes = [\"gender\", \"age\"]\n",
    "patient_infos = get_patient_infos()\n",
    "dict_all_setups, len_setups = get_attack_setups()\n",
    "counter = 1\n",
    "\n",
    "# Load the attack performance metrics if exists\n",
    "if os.path.exists(osj(\"..\", \"results_attacks\", f\"{attack}_performance.pkl\")):\n",
    "    aia_metrics = load_attack_performance(attack)\n",
    "else:\n",
    "    aia_metrics = {mechanism:{epsilon: {delta: {attribute: None for attribute in attributes} for delta in dict_all_setups[mechanism][epsilon]} for epsilon in dict_all_setups[mechanism]} for mechanism in dict_all_setups}\n",
    "\n",
    "for attribute in attributes:\n",
    "    counter = 1\n",
    "\n",
    "    attr_array = get_patient_attribute(attribute, patient_infos)\n",
    "\n",
    "    ###### PREPARE ITERATIONS ######\n",
    "    for mechanism in dict_all_setups:\n",
    "        ekg_loaded = False\n",
    "        last_epsilon = 0.0\n",
    "\n",
    "        for epsilon in dict_all_setups[mechanism]:\n",
    "\n",
    "            if epsilon <= 0.091:\n",
    "                file_epsilon = 0.091\n",
    "            elif epsilon <= 0.91:\n",
    "                file_epsilon = 0.91\n",
    "            elif epsilon <= 2.01:\n",
    "                file_epsilon = 2.01\n",
    "            \n",
    "            if epsilon not in aia_metrics[mechanism].keys():\n",
    "                aia_metrics[mechanism][epsilon] = {delta: {attribute: None for attribute in attributes} for delta in dict_all_setups[mechanism][epsilon]}\n",
    "\n",
    "            for delta in dict_all_setups[mechanism][epsilon]:\n",
    "\n",
    "                if delta not in aia_metrics[mechanism][epsilon].keys():\n",
    "                    aia_metrics[mechanism][epsilon][delta] = {attribute: None for attribute in attributes}\n",
    "                \n",
    "                attribute_metrics = {}\n",
    "\n",
    "                if aia_metrics[mechanism][epsilon][delta][attribute] != None:  # attribute was trained already\n",
    "                    logger.info(f\"{attribute}: Skipping existing inference for {mechanism}, {epsilon}, {delta} - ({counter}/{len_setups})\")\n",
    "                    counter += 1\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if file_epsilon != last_epsilon or ekg_loaded == False:\n",
    "                        ekg_loaded = True\n",
    "                        logger.info(f\"Loading {mechanism} data until epsilon {file_epsilon} (1-2 minutes) ...\")\n",
    "                        ekg_signals_dp = get_dp_signals(mechanism, file_epsilon)\n",
    "                    \n",
    "                    try:\n",
    "\n",
    "                        if attribute == \"gender\": \n",
    "                            \n",
    "                            counter += 1\n",
    "                            X, y = flatten_data(ekg_signals_dp[epsilon][delta], attr_array)\n",
    "                            logger.info(f\"Training {attribute} inference ({counter}/{len_setups})\")\n",
    "                            \n",
    "                            y = np.where(y == 'M', 0, 1)\n",
    "\n",
    "                            metrics = train_inference(X, y, binary=True)\n",
    "                            aia_metrics[mechanism][epsilon][delta][attribute] = metrics\n",
    "\n",
    "                        if attribute == \"age\":\n",
    "\n",
    "                            counter += 1\n",
    "                            X, y = flatten_data(ekg_signals_dp[epsilon][delta],attr_array)\n",
    "                            logger.info(f\"Training {attribute} inference ({counter} of {len_setups})\")\n",
    "                            \n",
    "                            # y has two entries with age \"-1\" which need to be removed\n",
    "                            valid_patients = y >= 0\n",
    "                            X = X[valid_patients]\n",
    "                            y = y[valid_patients]\n",
    "                            y_grouped = pd.cut(y, bins=[0, 40, 70, 100], labels=[\"0\", \"1\", \"2\"]).astype(int)\n",
    "\n",
    "                            metrics = train_inference(X, y_grouped, binary=False)\n",
    "                            aia_metrics[mechanism][epsilon][delta][attribute] = metrics\n",
    "\n",
    "\n",
    "                    except KeyError:\n",
    "                        logger.info(f\"{attribute}: No data available for {mechanism} with epsilon {epsilon} and delta {delta}.\")\n",
    "                        continue\n",
    "            \n",
    "            last_epsilon = file_epsilon\n",
    "\n",
    "    # save the metrics\n",
    "    save_attack_performance(attack, aia_metrics)\n",
    "    logger.info(f\"Saved attack performance for {attack}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'laplace': {0.91: {0.6: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.6,\n",
       "     'pre': 0.6,\n",
       "     'f1': 0.6,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}},\n",
       "   0.0: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.6,\n",
       "     'pre': 0.6,\n",
       "     'f1': 0.6,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}}},\n",
       "  0.81: {1.0: {'gender': {'acc': 0.7,\n",
       "     'rec': 0.8,\n",
       "     'pre': 0.6666666666666666,\n",
       "     'f1': 0.7272727272727273,\n",
       "     'mae': 0.3,\n",
       "     'mse': 0.3,\n",
       "     'r2': -0.19999999999999996},\n",
       "    'age': {'acc': 0.2,\n",
       "     'rec': 0.13333333333333333,\n",
       "     'pre': 0.09523809523809523,\n",
       "     'f1': 0.1111111111111111,\n",
       "     'mae': 0.8,\n",
       "     'mse': 0.8,\n",
       "     'r2': -0.9512195121951221}},\n",
       "   0.0: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.4444444444444444,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.71: {0.0: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.2,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.2857142857142857,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.61: {0.6: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.4,\n",
       "     'f1': 0.4,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}},\n",
       "   0.2: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.6666666666666666,\n",
       "     'f1': 0.5,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.1851851851851852,\n",
       "     'f1': 0.2380952380952381,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.8,\n",
       "     'r2': -0.9512195121951221}},\n",
       "   0.0: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.0,\n",
       "     'pre': 0.0,\n",
       "     'f1': 0.0,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.51: {0.0: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.0,\n",
       "     'pre': 0.0,\n",
       "     'f1': 0.0,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.3,\n",
       "     'rec': 0.19999999999999998,\n",
       "     'pre': 0.125,\n",
       "     'f1': 0.15384615384615385,\n",
       "     'mae': 0.7,\n",
       "     'mse': 0.7,\n",
       "     'r2': -0.7073170731707319}}},\n",
       "  0.081: {1.0: {'gender': {'acc': 0.7,\n",
       "     'rec': 0.8,\n",
       "     'pre': 0.6666666666666666,\n",
       "     'f1': 0.7272727272727273,\n",
       "     'mae': 0.3,\n",
       "     'mse': 0.3,\n",
       "     'r2': -0.19999999999999996},\n",
       "    'age': {'acc': 0.2,\n",
       "     'rec': 0.13333333333333333,\n",
       "     'pre': 0.09523809523809523,\n",
       "     'f1': 0.1111111111111111,\n",
       "     'mae': 0.8,\n",
       "     'mse': 0.8,\n",
       "     'r2': -0.9512195121951221}}},\n",
       "  0.071: {0.0: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.4444444444444444,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.6,\n",
       "     'rec': 0.4166666666666667,\n",
       "     'pre': 0.5185185185185185,\n",
       "     'f1': 0.37142857142857144,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': 0.024390243902438935}}},\n",
       "  0.061: {0.0: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.2,\n",
       "     'pre': 1.0,\n",
       "     'f1': 0.3333333333333333,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.20512820512820515,\n",
       "     'mae': 0.7,\n",
       "     'mse': 0.9,\n",
       "     'r2': -1.1951219512195124}}},\n",
       "  0.051: {0.8: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.4444444444444444,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}}},\n",
       "  0.041: {0.9: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.4444444444444444,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}},\n",
       "   0.0: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.0,\n",
       "     'pre': 0.0,\n",
       "     'f1': 0.0,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}}},\n",
       "  0.031: {0.1: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.2,\n",
       "     'pre': 1.0,\n",
       "     'f1': 0.3333333333333333,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.35000000000000003,\n",
       "     'pre': 0.3333333333333333,\n",
       "     'f1': 0.3162393162393162,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}},\n",
       "   0.0: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.0,\n",
       "     'pre': 0.0,\n",
       "     'f1': 0.0,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}}},\n",
       "  0.021: {0.1: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.0,\n",
       "     'pre': 0.0,\n",
       "     'f1': 0.0,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}},\n",
       "   0.0: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.2,\n",
       "     'pre': 1.0,\n",
       "     'f1': 0.3333333333333333,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}}},\n",
       "  0.01: {0.1: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.0,\n",
       "     'pre': 0.0,\n",
       "     'f1': 0.0,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}},\n",
       "   0.0: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.0,\n",
       "     'pre': 0.0,\n",
       "     'f1': 0.0,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.001: {0.1: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.2,\n",
       "     'pre': 1.0,\n",
       "     'f1': 0.3333333333333333,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}},\n",
       "   0.0: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.6666666666666666,\n",
       "     'f1': 0.5,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}}}},\n",
       " 'gaussian_a': {0.81: {0.9: {'gender': {'acc': 0.7,\n",
       "     'rec': 0.6,\n",
       "     'pre': 0.75,\n",
       "     'f1': 0.6666666666666666,\n",
       "     'mae': 0.3,\n",
       "     'mse': 0.3,\n",
       "     'r2': -0.19999999999999996},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}},\n",
       "   0.8: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.6,\n",
       "     'pre': 0.6,\n",
       "     'f1': 0.6,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.71: {0.8: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.4444444444444444,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}},\n",
       "   0.5: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.6,\n",
       "     'pre': 0.6,\n",
       "     'f1': 0.6,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}},\n",
       "   0.2: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.4444444444444444,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}}},\n",
       "  0.61: {0.9: {'gender': {'acc': 0.7,\n",
       "     'rec': 0.8,\n",
       "     'pre': 0.6666666666666666,\n",
       "     'f1': 0.7272727272727273,\n",
       "     'mae': 0.3,\n",
       "     'mse': 0.3,\n",
       "     'r2': -0.19999999999999996},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}},\n",
       "   0.3: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.4444444444444444,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.20512820512820515,\n",
       "     'mae': 0.7,\n",
       "     'mse': 0.9,\n",
       "     'r2': -1.1951219512195124}}},\n",
       "  0.51: {0.8: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.4444444444444444,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.3,\n",
       "     'rec': 0.19999999999999998,\n",
       "     'pre': 0.125,\n",
       "     'f1': 0.15384615384615385,\n",
       "     'mae': 0.7,\n",
       "     'mse': 0.7,\n",
       "     'r2': -0.7073170731707319}}},\n",
       "  0.41: {0.2: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.2,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.2857142857142857,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.091: {0.4: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.2,\n",
       "     'pre': 0.3333333333333333,\n",
       "     'f1': 0.25,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.081: {1.0: {'gender': {'acc': 0.7,\n",
       "     'rec': 0.8,\n",
       "     'pre': 0.6666666666666666,\n",
       "     'f1': 0.7272727272727273,\n",
       "     'mae': 0.3,\n",
       "     'mse': 0.3,\n",
       "     'r2': -0.19999999999999996},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}}},\n",
       "  0.071: {1.0: {'gender': {'acc': 0.7,\n",
       "     'rec': 0.8,\n",
       "     'pre': 0.6666666666666666,\n",
       "     'f1': 0.7272727272727273,\n",
       "     'mae': 0.3,\n",
       "     'mse': 0.3,\n",
       "     'r2': -0.19999999999999996},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}}},\n",
       "  0.051: {0.8: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.6,\n",
       "     'pre': 0.6,\n",
       "     'f1': 0.6,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.2,\n",
       "     'rec': 0.13333333333333333,\n",
       "     'pre': 0.09523809523809523,\n",
       "     'f1': 0.1111111111111111,\n",
       "     'mae': 0.8,\n",
       "     'mse': 0.8,\n",
       "     'r2': -0.9512195121951221}}},\n",
       "  0.041: {0.1: {'gender': {'acc': 0.6,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.6666666666666666,\n",
       "     'f1': 0.5,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': -0.6000000000000001},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.01: {0.8: {'gender': {'acc': 0.7,\n",
       "     'rec': 0.8,\n",
       "     'pre': 0.6666666666666666,\n",
       "     'f1': 0.7272727272727273,\n",
       "     'mae': 0.3,\n",
       "     'mse': 0.3,\n",
       "     'r2': -0.19999999999999996},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}},\n",
       "   0.5: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.2,\n",
       "     'pre': 0.3333333333333333,\n",
       "     'f1': 0.25,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}}},\n",
       "  0.001: {0.8: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.2,\n",
       "     'pre': 0.3333333333333333,\n",
       "     'f1': 0.25,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.3,\n",
       "     'rec': 0.19999999999999998,\n",
       "     'pre': 0.125,\n",
       "     'f1': 0.15384615384615385,\n",
       "     'mae': 0.7,\n",
       "     'mse': 0.7,\n",
       "     'r2': -0.7073170731707319}}}},\n",
       " 'bounded_n': {0.91: {0.2: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.4444444444444444,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.71: {0.2: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.4444444444444444,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.31: {0.2: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.2,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.2857142857142857,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.21: {0.2: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.2,\n",
       "     'pre': 0.3333333333333333,\n",
       "     'f1': 0.25,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.11: {0.3: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.6,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.5454545454545454,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.4,\n",
       "     'rec': 0.26666666666666666,\n",
       "     'pre': 0.14814814814814814,\n",
       "     'f1': 0.19047619047619047,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -0.46341463414634165}},\n",
       "   0.1: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.0,\n",
       "     'pre': 0.0,\n",
       "     'f1': 0.0,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.081: {0.2: {'gender': {'acc': 0.3,\n",
       "     'rec': 0.2,\n",
       "     'pre': 0.25,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.7,\n",
       "     'mse': 0.7,\n",
       "     'r2': -1.7999999999999998},\n",
       "    'age': {'acc': 0.6,\n",
       "     'rec': 0.4166666666666667,\n",
       "     'pre': 0.5185185185185185,\n",
       "     'f1': 0.37142857142857144,\n",
       "     'mae': 0.4,\n",
       "     'mse': 0.4,\n",
       "     'r2': 0.024390243902438935}}},\n",
       "  0.071: {0.4: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.2,\n",
       "     'pre': 0.3333333333333333,\n",
       "     'f1': 0.25,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}},\n",
       "   0.2: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.4,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.4444444444444444,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.021: {0.4: {'gender': {'acc': 0.4,\n",
       "     'rec': 0.2,\n",
       "     'pre': 0.3333333333333333,\n",
       "     'f1': 0.25,\n",
       "     'mae': 0.6,\n",
       "     'mse': 0.6,\n",
       "     'r2': -1.4},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.35000000000000003,\n",
       "     'pre': 0.3333333333333333,\n",
       "     'f1': 0.3162393162393162,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}},\n",
       "   0.1: {'gender': {'acc': 0.5,\n",
       "     'rec': 0.2,\n",
       "     'pre': 0.5,\n",
       "     'f1': 0.2857142857142857,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -1.0},\n",
       "    'age': {'acc': 0.5,\n",
       "     'rec': 0.3333333333333333,\n",
       "     'pre': 0.16666666666666666,\n",
       "     'f1': 0.2222222222222222,\n",
       "     'mae': 0.5,\n",
       "     'mse': 0.5,\n",
       "     'r2': -0.21951219512195141}}},\n",
       "  0.01: {0.1: {'gender': None, 'age': None}}},\n",
       " 'no_dp': {'gender': {'acc': 0.7,\n",
       "   'rec': 0.8,\n",
       "   'pre': 0.6666666666666666,\n",
       "   'f1': 0.7272727272727273,\n",
       "   'mae': 0.3,\n",
       "   'mse': 0.3,\n",
       "   'r2': -0.19999999999999996},\n",
       "  'age': {'acc': 0.2,\n",
       "   'rec': 0.13333333333333333,\n",
       "   'pre': 0.09523809523809523,\n",
       "   'f1': 0.1111111111111111,\n",
       "   'mae': 0.8,\n",
       "   'mse': 0.8,\n",
       "   'r2': -0.9512195121951221}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aia_metrics = load_attack_performance(\"AIA\")\n",
    "aia_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive: Manual Attack Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dp_signals(m, e):\n",
    "#     with open(osj(\"..\", \"data_dp\", f\"{m}_{e}.pkl\"), \"rb\") as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# def get_real_data():\n",
    "#     dict_samples = spio.loadmat('../data/s2s_mitbih_aami.mat')\n",
    "#     samples = dict_samples['s2s_mitbih']\n",
    "#     values = samples[0]['seg_values']\n",
    "#     return values\n",
    "\n",
    "# def get_patient_infos():\n",
    "#     with open(osj(\"..\", \"data\", \"patient_infos.pkl\"), \"rb\") as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# def get_ids():\n",
    "#     with open(osj(\"..\", \"data\", \"all_patients.pkl\"), \"rb\") as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# def get_patient_attribute(attribute, data):\n",
    "#     attr_dict = {str(pid): data[attribute] for pid, data in data.items()}\n",
    "#     attr_array = list(attr_dict.values())\n",
    "#     return attr_array\n",
    "\n",
    "# def prepare_inference_data(signals, attr_array, beats_per_patient=1000, beats_separated=True):\n",
    "#     \"\"\"\n",
    "#     Prepare the data for attribute inference attack.\n",
    "\n",
    "#     Parameters:\n",
    "#         signals: ndarray with structure [patient][segment][1][beat (280,)]\n",
    "#         attr_array: List or ndarray, target attribute\n",
    "#         beats_per_patient: int, Count of beats per patient (default: 1000)\n",
    "#         beats_separated: bool, if every beat is separated (default: True)\n",
    "\n",
    "#     Returns:\n",
    "#         X: ndarray in format [n_patients, beats_per_patient, beat_length] (if beats_separated=False)\n",
    "#            or [total_beats, beat_length] (if beats_separated=True)\n",
    "#         y: ndarray in format [n_patients] (if beats_separated=False)\n",
    "#            or [total_beats] (if beats_separated=True)\n",
    "#     \"\"\"\n",
    "#     X = []\n",
    "#     y = []\n",
    "\n",
    "#     for patient_idx in range(len(signals)):\n",
    "#         attr = attr_array[patient_idx]\n",
    "#         segments = signals[patient_idx] \n",
    "\n",
    "#         patient_beats = np.concatenate([segment[0] for segment in segments], axis=0)\n",
    "            \n",
    "#         # Sampling\n",
    "#         n_beats = min(beats_per_patient, len(patient_beats))\n",
    "#         sampled_beats = patient_beats[np.random.choice(len(patient_beats), n_beats, replace=False)]\n",
    "\n",
    "#         if beats_separated:\n",
    "#             X.append(sampled_beats)\n",
    "#             y.append(np.full(n_beats, attr))\n",
    "#         else:\n",
    "#             if len(sampled_beats) < beats_per_patient:\n",
    "#                 # Padding with zeros if less than beats_per_patient\n",
    "#                 padding = np.zeros((beats_per_patient - len(sampled_beats), sampled_beats.shape[1]))\n",
    "#                 sampled_beats = np.vstack((sampled_beats, padding))\n",
    "#             X.append(sampled_beats)\n",
    "#             y.append(attr)\n",
    "\n",
    "#     if beats_separated:\n",
    "#         X = np.concatenate(X, axis=0)  # shape: [n_beats, 280]\n",
    "#         y = np.concatenate(y, axis=0)  # shape: [n_beats]\n",
    "#     else:\n",
    "#         X = np.array(X) # shape: [n_patients, beats_per_patient, 280]\n",
    "#         y = np.array(y) # shape: [n_patients]\n",
    "    \n",
    "#     return X, y\n",
    "\n",
    "# def train_attribute_inference(signals, attr_array, attr_task=\"classification\"):\n",
    "#     \"\"\"\n",
    "#     signals: ndarray with structure [patient][segment][1][beat (280,)]\n",
    "#     patient_metadata: dict with structure {patient_index: {\"gender\": 0/1, ...}}\n",
    "#     attr_task: str, type of model to use for inference (default: \"randomForest\")\n",
    "\n",
    "#     Returns:\n",
    "#         y_test: real labels\n",
    "#         y_pred: predicted labels\n",
    "#         pred_classes: predicted classes (for classification tasks)\n",
    "#     \"\"\"\n",
    "\n",
    "#     if attr_task == \"classification\":\n",
    "\n",
    "#         X, y = prepare_inference_data(signals, attr_array, beats_per_patient=1000, beats_separated=True)\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "#         clf = RandomForestClassifier(\n",
    "#             n_estimators=100,\n",
    "#             max_depth=20,\n",
    "#             n_jobs=-1,\n",
    "#             random_state=42\n",
    "#         )\n",
    "\n",
    "#         clf.fit(X_train, y_train)\n",
    "#         y_pred = clf.predict(X_test)\n",
    "#         pred_classes = list(clf.classes_) \n",
    "\n",
    "#         return y_test, y_pred, pred_classes\n",
    "    \n",
    "#     elif attr_task == \"regression\":\n",
    "\n",
    "#         X, y = prepare_inference_data(signals, attr_array, beats_per_patient=1000, beats_separated=True)\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#         # xgb_model = XGBRegressor(\n",
    "#         #     tree_method=\"gpu_hist\",\n",
    "#         #     n_estimators=100,\n",
    "#         #     max_depth=10,\n",
    "#         #     learning_rate=0.1,\n",
    "#         #     random_state=42,\n",
    "#         #     n_jobs=-1\n",
    "#         # )\n",
    "#         # xgb_model = XGBRegressor(tree_method=\"gpu_hist\")\n",
    "#         # xgb_model.fit(X_train, y_train)\n",
    "#         # y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "#         # model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "#         # model.fit(X_train, y_train)\n",
    "#         # y_pred = model.predict(X_test)\n",
    "\n",
    "#         # model = LinearRegression()\n",
    "#         # model.fit(X_train, y_train)\n",
    "#         # y_pred = model.predict(X_test)\n",
    "\n",
    "#         model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "\n",
    "#         return y_test, y_pred\n",
    "    \n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported task type: {attr_task}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dp_signals(m, e):\n",
    "#     with open(osj(\"..\", \"data_dp\", f\"{m}_{e}.pkl\"), \"rb\") as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# def get_patient_infos():\n",
    "#     with open(osj(\"..\", \"data\", \"patient_infos.pkl\"), \"rb\") as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# def get_ids():\n",
    "#     with open(osj(\"..\", \"data\", \"all_patients.pkl\"), \"rb\") as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# def get_patient_attribute(attribute, data):\n",
    "#     attr_dict = {str(pid): data[attribute] for pid, data in data.items()}\n",
    "#     attr_array = list(attr_dict.values())\n",
    "#     return attr_array\n",
    "\n",
    "\n",
    "# def prepare_signal_for_inference(signal, attr_array, n_beats=1000, flat=\"patient\"):\n",
    "#     \"\"\"\n",
    "#     Prepare the signal for attribute inference attack.\n",
    "\n",
    "#     Parameters:\n",
    "#         signal: ndarray with structure [patient][segment][1][beat (280,)]\n",
    "#         attr_array: List or ndarray, target attribute\n",
    "#         n_beats: int, Count of beats per patient (default: 1000)\n",
    "#         flat: bool, if True, flatten the signal to [n_patients, n_values] (default: True)\n",
    "#               if False, flatten only by one dimension to [n_patients, n_segments, n_beats]\n",
    "\n",
    "#     Returns:\n",
    "#         X: ndarray in format [n_segments, 1, beat_length]\n",
    "#     \"\"\"\n",
    "\n",
    "#     # returns the signal in the format [n_patients, n_values]\n",
    "#     if flat == \"patient\":\n",
    "#         new_signal = {}\n",
    "#         for patient_idx in range(0, len(signal)): # 48\n",
    "#             all_values = []\n",
    "#             for segment_idx in range(min(n_beats, len(signal[patient_idx]))):\n",
    "#                 for value_idx in range(0, 280): #280 values per beat\n",
    "#                     all_values.append(signal[patient_idx][segment_idx][0][value_idx].item())\n",
    "                \n",
    "#             new_signal[patient_idx] = all_values\n",
    "        \n",
    "#         # requires no change to the attribute array, as it is already in the format [n_patients, attr] (48, 1)\n",
    "        \n",
    "\n",
    "#     # returns the signal in the format [n_patients, n_beats, n_values]\n",
    "#     elif flat == \"one_beat\":\n",
    "#         new_signal = {}\n",
    "#         for patient_idx in range(0, len(signal)): # 48\n",
    "#             all_values = [i for i in range(min(n_beats, len(signal[patient_idx])))]\n",
    "#             for segment_idx in range(min(n_beats, len(signal[patient_idx]))): \n",
    "#                 all_values[segment_idx] = signal[patient_idx][segment_idx][0]\n",
    "#             new_signal[patient_idx] = all_values\n",
    "        \n",
    "#         # reproduce attribute array to match the number of segments\n",
    "#         attr_array = np.array(attr_array)\n",
    "#         attr_segmented = [[attr[0]] * 1500 for attr in attr_array]\n",
    "\n",
    "#     # returns the signal in the format [n_beats, n_values] (200, 50 * 280)\n",
    "#     elif flat == \"50_beats\":\n",
    "#         new_signal = {}\n",
    "#         for patient_idx in range(0, len(signal)): # 48\n",
    "#             all_values = [i for i in range(min(n_beats, len(signal[patient_idx])))]\n",
    "#             for segment_idx in range(min(n_beats, len(signal[patient_idx]))): \n",
    "#                 all_values[segment_idx] = signal[patient_idx][segment_idx][0]\n",
    "#             new_signal[patient_idx] = all_values\n",
    "        \n",
    "#         # reproduce attribute array to match the number of segments\n",
    "#         attr_array = np.array(attr_array)\n",
    "#         attr_segmented = [[attr[0]] * 1500 for attr in attr_array]\n",
    "\n",
    "\n",
    "#     return new_signal\n",
    "\n",
    "\n",
    "# def prepare_inference_data(signals, attr_array, beats_per_patient=1000, beats_separated=True):\n",
    "#     \"\"\"\n",
    "#     Prepare the data for attribute inference attack.\n",
    "\n",
    "#     Parameters:\n",
    "#         signals: ndarray with structure [patient][segment][1][beat (280,)]\n",
    "#         attr_array: List or ndarray, target attribute\n",
    "#         beats_per_patient: int, Count of beats per patient (default: 1000)\n",
    "#         beats_separated: bool, if every beat is separated (default: True)\n",
    "\n",
    "#     Returns:\n",
    "#         X: ndarray in format [n_patients, beats_per_patient, beat_length] (if beats_separated=False)\n",
    "#            or [total_beats, beat_length] (if beats_separated=True)\n",
    "#         y: ndarray in format [n_patients] (if beats_separated=False)\n",
    "#            or [total_beats] (if beats_separated=True)\n",
    "#     \"\"\"\n",
    "#     X = []\n",
    "#     y = []\n",
    "\n",
    "#     for patient_idx in range(len(signals)):\n",
    "#         attr = attr_array[patient_idx]\n",
    "#         segments = signals[patient_idx] \n",
    "\n",
    "#         patient_beats = np.concatenate([segment[0] for segment in segments], axis=0)\n",
    "            \n",
    "#         # Sampling\n",
    "#         n_beats = min(beats_per_patient, len(patient_beats))\n",
    "#         sampled_beats = patient_beats[np.random.choice(len(patient_beats), n_beats, replace=False)]\n",
    "\n",
    "#         if beats_separated:\n",
    "#             X.append(sampled_beats)\n",
    "#             y.append(np.full(n_beats, attr))\n",
    "#         else:\n",
    "#             if len(sampled_beats) < beats_per_patient:\n",
    "#                 # Padding with zeros if less than beats_per_patient\n",
    "#                 padding = np.zeros((beats_per_patient - len(sampled_beats), sampled_beats.shape[1]))\n",
    "#                 sampled_beats = np.vstack((sampled_beats, padding))\n",
    "#             X.append(sampled_beats)\n",
    "#             y.append(attr)\n",
    "\n",
    "#     if beats_separated:\n",
    "#         X = np.concatenate(X, axis=0)  # shape: [n_beats, 280]\n",
    "#         y = np.concatenate(y, axis=0)  # shape: [n_beats]\n",
    "#     else:\n",
    "#         X = np.array(X) # shape: [n_patients, beats_per_patient, 280]\n",
    "#         y = np.array(y) # shape: [n_patients]\n",
    "    \n",
    "#     return X, y\n",
    "\n",
    "# def train_attribute_inference(signals, attr_array, attr_task=\"classification\"):\n",
    "#     \"\"\"\n",
    "#     signals: ndarray with structure [patient][segment][1][beat (280,)]\n",
    "#     patient_metadata: dict with structure {patient_index: {\"gender\": 0/1, ...}}\n",
    "#     attr_task: str, type of model to use for inference (default: \"randomForest\")\n",
    "\n",
    "#     Returns:\n",
    "#         y_test: real labels\n",
    "#         y_pred: predicted labels\n",
    "#         pred_classes: predicted classes (for classification tasks)\n",
    "#     \"\"\"\n",
    "\n",
    "#     if attr_task == \"classification\":\n",
    "\n",
    "#         X, y = prepare_inference_data(signals, attr_array, beats_per_patient=1000, beats_separated=True)\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "#         clf = RandomForestClassifier(\n",
    "#             n_estimators=100,\n",
    "#             max_depth=20,\n",
    "#             n_jobs=-1,\n",
    "#             random_state=42\n",
    "#         )\n",
    "\n",
    "#         clf.fit(X_train, y_train)\n",
    "#         y_pred = clf.predict(X_test)\n",
    "#         pred_classes = list(clf.classes_) \n",
    "\n",
    "#         return y_test, y_pred, pred_classes\n",
    "    \n",
    "#     elif attr_task == \"regression\":\n",
    "\n",
    "#         X, y = prepare_inference_data(signals, attr_array, beats_per_patient=1000, beats_separated=True)\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#         # xgb_model = XGBRegressor(\n",
    "#         #     tree_method=\"gpu_hist\",\n",
    "#         #     n_estimators=100,\n",
    "#         #     max_depth=10,\n",
    "#         #     learning_rate=0.1,\n",
    "#         #     random_state=42,\n",
    "#         #     n_jobs=-1\n",
    "#         # )\n",
    "#         # xgb_model = XGBRegressor(tree_method=\"gpu_hist\")\n",
    "#         # xgb_model.fit(X_train, y_train)\n",
    "#         # y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "#         # model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "#         # model.fit(X_train, y_train)\n",
    "#         # y_pred = model.predict(X_test)\n",
    "\n",
    "#         # model = LinearRegression()\n",
    "#         # model.fit(X_train, y_train)\n",
    "#         # y_pred = model.predict(X_test)\n",
    "\n",
    "#         model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "\n",
    "#         return y_test, y_pred\n",
    "    \n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported task type: {attr_task}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST GENDER INFERENCE\n",
    "# patient_infos = get_patient_infos()\n",
    "# attr_array = get_patient_attribute(\"gender\", patient_infos)\n",
    "\n",
    "# y_test, y_pred, pred_classes = train_attribute_inference(real_signals, attr_array, task=\"randomforest\")\n",
    "\n",
    "# print(classification_report(y_test, y_pred, target_names=pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST AGE INFERENCE - CLASSIFICATION\n",
    "# patient_infos = get_patient_infos()\n",
    "# attr_array = get_patient_attribute(\"age\", patient_infos)\n",
    "\n",
    "# # patient 4 (idx = 3) and 38 (idx = 37, will be 36) have no age and therefore will be removed\n",
    "# del attr_array[3]\n",
    "# real_signals_age = np.delete(real_signals, 3, axis=0) \n",
    "# del attr_array[36]\n",
    "# real_signals_age = np.delete(real_signals_age, 36, axis=0) \n",
    "\n",
    "# print(attr_array.value_counts())\n",
    "\n",
    "# attr_array = pd.cut(attr_array, bins=[0, 60, 70, 100], labels=[\"adult\", \"older\", \"senior\"])\n",
    "\n",
    "# y_test, y_pred, pred_classes = train_attribute_inference(real_signals_age, attr_array, task=\"randomforest\")\n",
    "\n",
    "# print(classification_report(y_test, y_pred, target_names=pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST AGE INFERENCE - REGRESSION\n",
    "# patient_infos = get_patient_infos()\n",
    "# attr_array = get_patient_attribute(\"age\", patient_infos)\n",
    "\n",
    "# # patient 4 (idx = 3) and 38 (idx = 37, will be 36) have no age and therefore will be removed\n",
    "# del attr_array[3]\n",
    "# real_signals_age = np.delete(real_signals, 3, axis=0) \n",
    "# del attr_array[36]\n",
    "# real_signals_age = np.delete(real_signals_age, 36, axis=0) \n",
    "\n",
    "# y_test, y_pred = train_attribute_inference(real_signals_age, attr_array, task=\"regression\")\n",
    "\n",
    "# print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))\n",
    "# print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "# print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# # Results GradientBoostingRegressor:\n",
    "# # Mean Absolute Error (MAE): 13.795175370909545\n",
    "# # Mean Squared Error (MSE): 311.87486641343287\n",
    "# # R² Score: -0.0013360272321962796\n",
    "\n",
    "# # Results LinearRegression:\n",
    "# # Mean Absolute Error (MAE): 13.746101115734675\n",
    "# # Mean Squared Error (MSE): 311.4475515455648\n",
    "# # R² Score: 3.595178326243342e-05\n",
    "\n",
    "# # Results MLPRegressor:\n",
    "# # Mean Absolute Error (MAE): 13.765186316133146\n",
    "# # Mean Squared Error (MSE): 310.87597922384793\n",
    "# # R² Score: 0.0018710979253216964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mechanism = \"laplace\"\n",
    "# file_epsilon = 0.091\n",
    "# epsilon = 0.01\n",
    "# delta = 0.5\n",
    "\n",
    "# inference_attributes = [\"gender\", \"age\", \"patient_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GET DP X\n",
    "# ekg_signals_dp = get_dp_signals(mechanism, file_epsilon)\n",
    "# dp_signals = ekg_signals_dp[epsilon][delta]\n",
    "# del ekg_signals_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GET Y\n",
    "# patient_infos = get_patient_infos()\n",
    "# attr_array = get_patient_attribute(\"gender\", patient_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flatten_data(signals, attr_array):\n",
    "#     \"\"\"\n",
    "#     Flatten the signals to have all beats of a patient in one array.\n",
    "\n",
    "#     Parameters:\n",
    "#         signals: ndarray with structure [patient][segment][1][beat (280,)]\n",
    "#         attr_array: List or ndarray, target attribute\n",
    "\n",
    "#     Returns:\n",
    "#         X: ndarray in format [n_patients, n_values]\n",
    "#         y: ndarray in format [n_patients]\n",
    "#     \"\"\"\n",
    "#     # PREPARE X\n",
    "#     n_beats = 1500\n",
    "#     beat_length = 280\n",
    "\n",
    "#     new_signal = {}\n",
    "#     for patient_idx in range(0, len(signals)): # 48\n",
    "\n",
    "#         all_values = []\n",
    "#         for beat_idx in range(n_beats):\n",
    "#             for value_idx in range(0, beat_length): #280 values per beat\n",
    "#                 all_values.append(signals[patient_idx][beat_idx][0][value_idx].item())\n",
    "            \n",
    "#         new_signal[patient_idx] = all_values\n",
    "    \n",
    "#     signal_array = np.array(list(new_signal.values()))\n",
    "#     signal_array.shape\n",
    "#     X = signal_array\n",
    "    \n",
    "#     # PREPARE Y\n",
    "#     attr_array = np.array(attr_array)\n",
    "#     attr_array.shape\n",
    "#     y = attr_array\n",
    "\n",
    "#     return X, y   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming both, X and Y into 2 dimensional arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attr_array = np.array(attr_array)\n",
    "# attr_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 420000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signal_array = np.array(list(new_signal.values()))\n",
    "# signal_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = signal_array\n",
    "# y = attr_array\n",
    "# y = np.where(y == 'M', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_gender_inference(X, y):\n",
    "#     \"\"\"\n",
    "#     Trains gender inference model using Random Forest Classifier.\n",
    "\n",
    "#     Parameters:\n",
    "#         X: ndarray with shape [n_patients, n_values]\n",
    "#         y: ndarray with shape [n_patients]\n",
    "\n",
    "#     Returns:\n",
    "#         metrics: dict with accuracy, precision, recall, f1-score, mae, mse, r2\n",
    "#     \"\"\"\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#     clf = RandomForestClassifier(\n",
    "#         n_estimators=100,\n",
    "#         max_depth=None,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X_test)\n",
    "\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "#     pre, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "#     mae = mean_absolute_error(y_test, y_pred)\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "#     # save the metrics\n",
    "#     metrics = {\n",
    "#         \"acc\": acc,\n",
    "#         \"rec\": rec,\n",
    "#         \"pre\": pre,\n",
    "#         \"f1\": f1,\n",
    "#         \"mae\": mae,\n",
    "#         \"mse\": mse,\n",
    "#         \"r2\": r2\n",
    "#     }\n",
    "\n",
    "#     return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.4,\n",
       " 'rec': 0.2,\n",
       " 'pre': 0.3333333333333333,\n",
       " 'f1': 0.25,\n",
       " 'mae': 0.6,\n",
       " 'mse': 0.6,\n",
       " 'r2': -1.4}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_segmented = np.repeat(attr_array, 1500, axis=0)\n",
    "# labels_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_signals = dp_signals.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beat_signal = {}\n",
    "# for patient in range(0, len(test_signals)): # 48\n",
    "#     all_values = [i for i in range(len(test_signals[patient]))]\n",
    "#     for segment in range(len(test_signals[patient])): # individual per patient but for all min. 1500\n",
    "#         all_values[segment] = test_signals[patient][segment][0]\n",
    "#     beat_signal[patient] = all_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive: Setup for Membership Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_real_train_test_data():\n",
    "#     dict_samples = spio.loadmat('../data/s2s_mitbih_aami_DS1DS2.mat')\n",
    "\n",
    "#     DS1_samples = dict_samples['s2s_mitbih_DS1']\n",
    "#     DS1_values = DS1_samples[0]['seg_values']\n",
    "#     DS1_labels = DS1_samples[0]['seg_labels']\n",
    "\n",
    "#     DS2_samples = dict_samples['s2s_mitbih_DS2']\n",
    "#     DS2_values = DS2_samples[0]['seg_values']\n",
    "#     DS2_labels = DS2_samples[0]['seg_labels']\n",
    "\n",
    "#     DS1_values, DS1_labels = prepare_data(DS1_values, DS1_labels)\n",
    "#     DS2_values, DS2_labels = prepare_data(DS2_values, DS2_labels)\n",
    "\n",
    "#     return DS1_values, DS1_labels, DS2_values, DS2_labels\n",
    "\n",
    "# def load_train_test_data(m, e):\n",
    "#     with open(osj(\"..\", \"dp_models\", \"train_test_data\", m, f\"{e}_data.pkl\"), \"rb\") as f:\n",
    "#         data = pickle.load(f)\n",
    "#     return data \n",
    "\n",
    "# def prepare_data(values, labels, max_time=100, classes=['N', 'S', 'V'], max_label=100):\n",
    "\n",
    "#     # calculate the number of annotations and sequences\n",
    "#     num_annots = sum([item.shape[0] for item in temp_values]) \n",
    "#     n_seqs = num_annots / max_time\n",
    "\n",
    "#     # add all beats together\n",
    "#     count_b = 0\n",
    "#     nr_recordings = [] # number of recordings per patient (each recording contains 280 measurements)\n",
    "#     for _, item in enumerate(temp_values):\n",
    "#         l = item.shape[0] # number of recordings per patient (each recording contains 280 measurements)\n",
    "#         nr_recordings.append(l)\n",
    "#         for itm in item:\n",
    "#             if count_b == num_annots: # hence all recordings have been added\n",
    "#                 break\n",
    "#             beats.append(itm[0]) # itm is one recording, with 280 measurements\n",
    "#             count_b += 1\n",
    "\n",
    "#     # add all labels together\n",
    "#     count_l  = 0\n",
    "#     t_labels = []\n",
    "#     for _, item in enumerate(labels): \n",
    "#         if len(t_labels) == num_annots: # break if all labels have been added\n",
    "#             break\n",
    "#         item = item[0]\n",
    "#         # iterate over all recordings per patient\n",
    "#         for lbl in item: \n",
    "#             if count_l == num_annots: # break if all labels have been added\n",
    "#                 break\n",
    "#             t_labels.append(str(lbl))\n",
    "#             count_l += 1\n",
    "    \n",
    "#     del temp_values\n",
    "#     # convert list to array & reshape\n",
    "#     beats = np.asarray(beats)\n",
    "#     t_labels = np.asarray(t_labels)  \n",
    "#     shape_v = beats.shape # 109338 rows with each 280 entries (109338, 280, 1)\n",
    "#     beats = np.reshape(beats, [shape_v[0], -1]) # new shape = (109338, 280)\n",
    "\n",
    "#     # Create empty arrays for data and labels\n",
    "#     random_beats  = np.asarray([],dtype=np.float64).reshape(0,shape_v[1])\n",
    "#     random_labels = np.asarray([],dtype=np.dtype('|S1')).reshape(0,)\n",
    "\n",
    "#     # iterate over all classes and truncate to max_label samples, so that all classes are equally represented\n",
    "#     for cl in classes:\n",
    "#         _label = np.where(t_labels == cl) # select indices that match the class\n",
    "#         logger.info(f\"Class {cl} is represented {len(_label[0])}\")\n",
    "\n",
    "#         # random permutation of indices\n",
    "#         permute = np.random.permutation(len(_label[0])) \n",
    "#         _label = _label[0][permute[:max_label]] # choose the first X indices\n",
    "#         logger.info(f\"Class {cl} is now represented {len(_label)}\")\n",
    "\n",
    "#         random_beats = np.concatenate((random_beats, beats[_label]))\n",
    "#         random_labels = np.concatenate((random_labels, t_labels[_label]))\n",
    "\n",
    "#     # shorten data to multiple of max_time\n",
    "#     signals = random_beats[:int(len(random_beats)/ max_time) * max_time, :]\n",
    "#     _labels  = random_labels[:int(len(random_beats) / max_time) * max_time]\n",
    "\n",
    "#     #  reshape data into groups of max_time\n",
    "#     data   = [signals[i:i + max_time] for i in range(0, len(signals), max_time)]\n",
    "#     labels = [_labels[i:i + max_time] for i in range(0, len(_labels), max_time)]\n",
    "\n",
    "#     permute = np.random.permutation(len(labels)) # random permutation of indices only\n",
    "\n",
    "#     # transform from list to array\n",
    "#     data   = np.asarray(data, dtype=object) \n",
    "#     labels = np.asarray(labels, dtype=object)\n",
    "\n",
    "#     # reorder data and labels according to random permute\n",
    "#     data   = data[permute]\n",
    "#     labels = labels[permute]\n",
    "\n",
    "#     logger.info('Signals and labels processed!')\n",
    "\n",
    "#     return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # F1 Read files\n",
    "# def read_data(filename, values, max_time=100, classes=['N', 'S', 'V'], max_label=100, trainset=1):\n",
    "\n",
    "#     random.seed(654)\n",
    "#     beats = [] \n",
    "#     dict_samples = spio.loadmat(filename + '.mat')\n",
    "#     samples = dict_samples['s2s_mitbih'] # 2D array with 2 columns: ecg values and labels\n",
    "#     labels = samples[0]['seg_labels'] # labels\n",
    "\n",
    "#     # patient IDs for train / test set\n",
    "#     # DS1 = [101, 106, 108, 109, 112, 114, 115, 116, 118,119, 122, 124, 201, 203, 205, 207, 208, 209, 215, 220, 223,230];\n",
    "#     # DS2 = [100, 103, 105, 111, 113, 117, 121, 123,200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233,234];\n",
    "#     DS1_idx = [1, 6, 8,  9, 11, 13, 14, 15, 17, 18, 20, 22, 24, 26, 27, 28, 29, 30, 35, 38, 41, 43]\n",
    "#     DS2_idx = [0, 3, 5, 10, 12, 16, 19, 21, 23, 25, 31, 32, 33, 34, 37, 39, 40, 42, 44, 45, 46, 47]\n",
    "\n",
    "#     # Select train and test data\n",
    "#     if trainset == 1:\n",
    "#         temp_values = [values[i] for i in DS1_idx]\n",
    "#         labels = [labels[i] for i in DS1_idx]\n",
    "\n",
    "#     elif trainset == 0:\n",
    "#         temp_values = [values[i] for i in DS2_idx]\n",
    "#         labels = [labels[i] for i in DS2_idx]\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
