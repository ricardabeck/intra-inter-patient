{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 07:41:19.294557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744875679.310478 2691937 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744875679.315397 2691937 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-17 07:41:19.332730: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/rbeck1_sw/inter-intra-patient/venv_tf/lib/python3.12/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/rbeck1_sw/inter-intra-patient/venv_tf/lib/python3.12/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.18.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as spio\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from os.path import join as osj\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Embedding, Dense, Bidirectional, Input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from  sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "random.seed(654)\n",
    "\n",
    "def read_inter_top10():\n",
    "    inter_top_10 = pd.read_csv('../results_dp/inter_top_10.csv')\n",
    "    inter_low_10 = pd.read_csv('../results_dp/inter_low_10.csv')\n",
    "    return inter_top_10, inter_low_10\n",
    "\n",
    "def read_dp_signals(m, e):\n",
    "    with open(osj(\"..\", \"data_dp\", f\"{m}_{e}.pkl\"), \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def read_results():\n",
    "    with open(osj(\"..\", \"results_shadow\", \"interpatient_results.pkl\"), \"rb\") as f:\n",
    "        results =  pickle.load(f)\n",
    "    with open(osj(\"..\", \"results_shadow\", \"interpatient_loss.pkl\"), \"rb\") as f:\n",
    "        loss =  pickle.load(f)\n",
    "    return results, loss\n",
    "\n",
    "# F1 Read files\n",
    "def read_data(filename, values, max_time=100, classes=['N', 'S', 'V'], max_label=100, trainset=1):\n",
    "\n",
    "    random.seed(654)\n",
    "    beats = [] \n",
    "    dict_samples = spio.loadmat(filename + '.mat')\n",
    "    samples = dict_samples['s2s_mitbih']\n",
    "    labels = samples[0]['seg_labels']\n",
    "\n",
    "    # patient IDs for original train / test set\n",
    "    # Train set: DS1 = [101, 106, 108, 109, 112, 114, 115, 116, 118,119, 122, 124, 201, 203, 205, 207, 208, 209, 215, 220, 223,230];\n",
    "    # Test set: DS2 = [100, 103, 105, 111, 113, 117, 121, 123,200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233,234];\n",
    "    # assuming the test set was leaked or is publicly available\n",
    "    #DS1_idx = [1, 6, 8,  9, 11, 13, 14, 15, 17, 18, 20, 22, 24, 26, 27, 28, 29, 30, 35, 38, 41, 43]\n",
    "    DS2_shadow_train = [0,  5, 12, 19, 23, 31, 33, 37, 44, 45, 46]\n",
    "    DS2_shadow_test  = [3, 10, 16, 21, 25, 32, 34, 39, 40, 42, 47]\n",
    "\n",
    "    # Select train and test data\n",
    "    if trainset == 1:\n",
    "        temp_values = [values[i] for i in DS2_shadow_train]\n",
    "        labels = [labels[i] for i in DS2_shadow_train]\n",
    "\n",
    "    elif trainset == 0:\n",
    "        temp_values = [values[i] for i in DS2_shadow_test]\n",
    "        labels = [labels[i] for i in DS2_shadow_test]\n",
    "\n",
    "    # calculate the number of annotations and sequences\n",
    "    num_annots = sum([item.shape[0] for item in temp_values])\n",
    "    n_seqs = num_annots / max_time \n",
    "\n",
    "    # add all beats together\n",
    "    count_b = 0\n",
    "    nr_recordings = [] # number of recordings per patient (each recording contains 280 measurements)\n",
    "    for _, item in enumerate(temp_values):\n",
    "        l = item.shape[0] # number of recordings per patient (each recording contains 280 measurements)\n",
    "        nr_recordings.append(l)\n",
    "        for itm in item:\n",
    "            if count_b == num_annots: # hence all recordings have been added\n",
    "                break\n",
    "            beats.append(itm[0]) # itm is one recording, with 280 measurements\n",
    "            count_b += 1\n",
    "\n",
    "    # add all labels together\n",
    "    count_l  = 0\n",
    "    t_labels = []\n",
    "    for _, item in enumerate(labels): \n",
    "        if len(t_labels) == num_annots: # break if all labels have been added\n",
    "            break\n",
    "        item = item[0]\n",
    "        # iterate over all recordings per patient\n",
    "        for lbl in item: \n",
    "            if count_l == num_annots: # break if all labels have been added\n",
    "                break\n",
    "            t_labels.append(str(lbl))\n",
    "            count_l += 1\n",
    "    \n",
    "    del temp_values\n",
    "    # convert list to array & reshape\n",
    "    beats = np.asarray(beats)\n",
    "    t_labels = np.asarray(t_labels)  \n",
    "    shape_v = beats.shape\n",
    "    beats = np.reshape(beats, [shape_v[0], -1])\n",
    "\n",
    "    # Create empty arrays for data and labels\n",
    "    random_beats  = np.asarray([],dtype=np.float64).reshape(0,shape_v[1])\n",
    "    random_labels = np.asarray([],dtype=np.dtype('|S1')).reshape(0,)\n",
    "\n",
    "    # iterate over all classes and truncate to max_label samples, so that all classes are equally represented\n",
    "    for cl in classes:\n",
    "        _label = np.where(t_labels == cl) # select indices that match the class\n",
    "        logger.info(f\"Class {cl} is represented {len(_label[0])}\")\n",
    "\n",
    "        # random permutation of indices\n",
    "        permute = np.random.permutation(len(_label[0])) \n",
    "        _label = _label[0][permute[:max_label]] # choose the first X indices\n",
    "        logger.info(f\"Class {cl} is now represented {len(_label)}\")\n",
    "\n",
    "        random_beats = np.concatenate((random_beats, beats[_label]))\n",
    "        random_labels = np.concatenate((random_labels, t_labels[_label]))\n",
    "\n",
    "    # shorten data to multiple of max_time\n",
    "    signals = random_beats[:int(len(random_beats)/ max_time) * max_time, :]\n",
    "    _labels  = random_labels[:int(len(random_beats) / max_time) * max_time]\n",
    "\n",
    "    #  reshape data into groups of max_time\n",
    "    data   = [signals[i:i + max_time] for i in range(0, len(signals), max_time)]\n",
    "    labels = [_labels[i:i + max_time] for i in range(0, len(_labels), max_time)]\n",
    "\n",
    "    permute = np.random.permutation(len(labels)) # random permutation of indices only\n",
    "\n",
    "    # transform from list to array\n",
    "    data   = np.asarray(data, dtype=object) \n",
    "    labels = np.asarray(labels, dtype=object)\n",
    "\n",
    "    # reorder data and labels according to random permute\n",
    "    data   = data[permute]\n",
    "    labels = labels[permute]\n",
    "\n",
    "    logger.info('Signals and labels processed!')\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# F2 Normaliza data\n",
    "def normalize(data):\n",
    "        data = np.nan_to_num(data)  # removing NaNs and Infs\n",
    "        data = data - np.mean(data)\n",
    "        data = data / np.std(data)\n",
    "        return data\n",
    "\n",
    "# F3 shuffle\n",
    "def batch_data(x, y, batch_size):\n",
    "    shuffle = np.random.permutation(len(x))\n",
    "    start = 0\n",
    "    #     from IPython.core.debugger import Tracer; Tracer()()\n",
    "    x = x[shuffle]\n",
    "    y = y[shuffle]\n",
    "    while start + batch_size <= len(x):\n",
    "        yield x[start:start + batch_size], y[start:start + batch_size]\n",
    "        start += batch_size\n",
    "\n",
    "# F4 bool value check\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "# F5 calculate performance\n",
    "def evaluate_metrics(confusion_matrix):\n",
    "    # https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP / (TP + FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN / (TN + FP)\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP / (TP + FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN / (TN + FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP / (FP + TN)\n",
    "    # False negative rate\n",
    "    FNR = FN / (TP + FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP / (TP + FP)\n",
    "\n",
    "    # Overall accuracy\n",
    "    ACC = (TP + TN) / (TP + FP + FN + TN)\n",
    "    # ACC_micro = (sum(TP) + sum(TN)) / (sum(TP) + sum(FP) + sum(FN) + sum(TN))\n",
    "    ACC_macro = np.mean(ACC) # to get a sense of effectiveness of our method on the small classes we computed this average (macro-average)\n",
    "\n",
    "    # F1 Score (Harmonic Mean of Precision & Recall)\n",
    "    F1 = 2 * (PPV * TPR) / (PPV + TPR)\n",
    "\n",
    "    return ACC_macro, ACC, TPR, TNR, PPV, NPV, FPR, FNR, FDR, F1\n",
    "\n",
    "# F6 Network\n",
    "def build_network(inputs, dec_inputs, char2numY, n_channels=10, input_depth=280, num_units=128, max_time=10, bidirectional=False):\n",
    "    # Reshape the inputs to match the Conv1D expected shape\n",
    "    _inputs = tf.reshape(inputs, [-1, n_channels, int(input_depth / n_channels)])\n",
    "    \n",
    "    # Convolutional and MaxPooling layers\n",
    "    conv1 = Conv1D(filters=32, kernel_size=2, strides=1, padding='same', activation='relu')(_inputs)\n",
    "    max_pool_1 = MaxPooling1D(pool_size=2, strides=2, padding='same')(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters=64, kernel_size=2, strides=1, padding='same', activation='relu')(max_pool_1)\n",
    "    max_pool_2 = MaxPooling1D(pool_size=2, strides=2, padding='same')(conv2)\n",
    "\n",
    "    conv3 = Conv1D(filters=128, kernel_size=2, strides=1, padding='same', activation='relu')(max_pool_2)\n",
    "\n",
    "    # Flatten the output of the Conv1D layers\n",
    "    shape = conv3.shape.as_list()\n",
    "    data_input_embed = tf.reshape(conv3, (-1, max_time, shape[1] * shape[2]))\n",
    "\n",
    "    # Embedding for the decoder\n",
    "    embed_size = 10\n",
    "    output_embedding = tf.Variable(tf.random.uniform((len(char2numY), embed_size), -1.0, 1.0), name='dec_embedding')\n",
    "    data_output_embed = tf.nn.embedding_lookup(params=output_embedding, ids=dec_inputs)\n",
    "\n",
    "    # Encoder\n",
    "    if not bidirectional:\n",
    "        # Regular LSTM\n",
    "        lstm_enc = LSTM(num_units, return_state=True)\n",
    "        _, last_state_h, last_state_c = lstm_enc(data_input_embed)\n",
    "        last_state = (last_state_c, last_state_h)\n",
    "    else:\n",
    "        # Bidirectional LSTM\n",
    "        lstm_enc = Bidirectional(LSTM(num_units, return_state=True))\n",
    "        _, forward_h, forward_c, backward_h, backward_c = lstm_enc(data_input_embed)\n",
    "        last_state_c = tf.concat([forward_c, backward_c], axis=-1)\n",
    "        last_state_h = tf.concat([forward_h, backward_h], axis=-1)\n",
    "        last_state = (last_state_c, last_state_h)\n",
    "\n",
    "    # Decoder\n",
    "    if not bidirectional:\n",
    "        lstm_dec = LSTM(num_units, return_sequences=True)\n",
    "    else:\n",
    "        lstm_dec = LSTM(2 * num_units, return_sequences=True)\n",
    "\n",
    "    dec_outputs = lstm_dec(data_output_embed, initial_state=last_state)\n",
    "\n",
    "    # Final Dense layer to produce logits\n",
    "    logits = Dense(len(char2numY))(dec_outputs)\n",
    "\n",
    "    return logits\n",
    "\n",
    "# 7 Model evaluation\n",
    "def test_model(sess, logits, X_test, y_test, batch_size, char2numY, y_seq_length, inputs, dec_inputs):\n",
    "    # source_batch, target_batch = next(batch_data(X_test, y_test, batch_size))\n",
    "    acc_track = []\n",
    "    sum_test_conf = []\n",
    "    count = 0\n",
    "    for batch_i, (source_batch, target_batch) in enumerate(batch_data(X_test, y_test, batch_size)):\n",
    "        #logger.info(f\"Running batch: {count}\")\n",
    "        count = count + 1\n",
    "        dec_input = np.zeros((len(source_batch), 1)) + char2numY['<GO>']\n",
    "\n",
    "        for i in range(y_seq_length):\n",
    "            batch_logits = sess.run(logits, feed_dict={inputs: source_batch, dec_inputs: dec_input})\n",
    "            prediction = batch_logits[:, -1].argmax(axis=-1)\n",
    "            dec_input = np.hstack([dec_input, prediction[:, None]])\n",
    "\n",
    "        acc_track.append(dec_input[:, 1:] == target_batch[:, 1:])\n",
    "        y_true= target_batch[:, 1:].flatten()\n",
    "        y_pred = dec_input[:, 1:].flatten()\n",
    "        sum_test_conf.append(confusion_matrix(y_true, y_pred,labels=list(range(len(char2numY)-1))))\n",
    "\n",
    "    sum_test_conf= np.mean(np.array(sum_test_conf, dtype=np.float32), axis=0)\n",
    "    acc_avg, acc, sensitivity, specificity, ppv, npv, fpr, fnr, fdr, f1_score = evaluate_metrics(sum_test_conf)\n",
    "\n",
    "    logger.info(f\"Average Accuracy is: {acc_avg} on test set\")\n",
    "\n",
    "    # for idx in range(n_classes):\n",
    "        # logger.info(f\"\\t{classes[idx]} rhythm -> Sensitivity: {sensitivity[idx]}, Specificity : {specificity[idx]}, Precision (PPV) : {ppv[idx]}, Accuracy : {acc[idx]}\")\n",
    "\n",
    "    logger.info(f\"\\t Average -> Sensitivity: {np.mean(sensitivity)}, Specificity : {np.mean(specificity)}, Precision (PPV) : {np.mean(ppv)}, Accuracy : {np.mean(acc)}\")\n",
    "    \n",
    "    return acc_avg, acc, sensitivity, specificity, ppv, npv, fpr, fnr, fdr, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "max_time = 20\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "num_units = 128\n",
    "bidirectional = True\n",
    "# lstm_layers = args.lstm_layers\n",
    "n_oversampling = 10000\n",
    "checkpoint_dir = 'dp_checkpoints_DS/shadow/checkpoints'\n",
    "ckpt_name = 'seq2seq.ckpt'\n",
    "test_steps = 10\n",
    "classes= ['N','S','V']\n",
    "filename = '../data/s2s_mitbih_aami'\n",
    "n_channels = 10\n",
    "\n",
    "# DP Combinations - Top and Low 10\n",
    "inter_top_10, inter_low_10 = read_inter_top10()\n",
    "file_epsilon = 0.091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Epsilon",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Delta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Epoch",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Metric",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "N",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "class_average",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2a704645-0d9b-44e5-9c52-e798ceac4799",
       "rows": [
        [
         "0",
         "Inter-laplace",
         "0.041",
         "0.9",
         "99",
         "f1_score",
         "0.99638593",
         "0.91279846",
         "0.9990634",
         "0.96941596"
        ],
        [
         "3",
         "Inter-laplace",
         "0.021",
         "0.6",
         "99",
         "f1_score",
         "0.99499434",
         "0.8843088",
         "0.9939638",
         "0.9577556"
        ],
        [
         "1",
         "Inter-gaussian_a",
         "0.01",
         "0.8",
         "99",
         "f1_score",
         "0.99550176",
         "0.8896682",
         "0.9944065",
         "0.95985883"
        ],
        [
         "2",
         "Inter-gaussian_a",
         "0.041",
         "0.4",
         "99",
         "f1_score",
         "0.9946434",
         "0.8912123",
         "0.9934924",
         "0.95978266"
        ],
        [
         "4",
         "Inter-gaussian_a",
         "0.031",
         "0.9",
         "99",
         "f1_score",
         "0.99404114",
         "0.8709036",
         "0.99751395",
         "0.9541529"
        ],
        [
         "5",
         "Inter-gaussian_a",
         "0.01",
         "0.4",
         "99",
         "f1_score",
         "0.9948045",
         "0.87079835",
         "0.98788446",
         "0.9511624"
        ],
        [
         "7",
         "Inter-gaussian_a",
         "0.021",
         "0.4",
         "99",
         "f1_score",
         "0.9957545",
         "0.86563754",
         "0.9851689",
         "0.9488537"
        ],
        [
         "8",
         "Inter-gaussian_a",
         "0.041",
         "0.2",
         "99",
         "f1_score",
         "0.99341005",
         "0.85656154",
         "0.9962825",
         "0.9487514"
        ],
        [
         "9",
         "Inter-gaussian_a",
         "0.021",
         "0.5",
         "99",
         "f1_score",
         "0.9940017",
         "0.85619104",
         "0.99565214",
         "0.948615"
        ],
        [
         "6",
         "Inter-bounded_n",
         "0.01",
         "0.3",
         "99",
         "f1_score",
         "0.99329716",
         "0.85721266",
         "0.997359",
         "0.9492896"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Metric</th>\n",
       "      <th>N</th>\n",
       "      <th>S</th>\n",
       "      <th>V</th>\n",
       "      <th>class_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inter-laplace</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.9</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.996386</td>\n",
       "      <td>0.912798</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>0.969416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inter-laplace</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.6</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.994994</td>\n",
       "      <td>0.884309</td>\n",
       "      <td>0.993964</td>\n",
       "      <td>0.957756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.995502</td>\n",
       "      <td>0.889668</td>\n",
       "      <td>0.994406</td>\n",
       "      <td>0.959859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.4</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.994643</td>\n",
       "      <td>0.891212</td>\n",
       "      <td>0.993492</td>\n",
       "      <td>0.959783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.9</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.994041</td>\n",
       "      <td>0.870904</td>\n",
       "      <td>0.997514</td>\n",
       "      <td>0.954153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.4</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.994804</td>\n",
       "      <td>0.870798</td>\n",
       "      <td>0.987884</td>\n",
       "      <td>0.951162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.4</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.995754</td>\n",
       "      <td>0.865638</td>\n",
       "      <td>0.985169</td>\n",
       "      <td>0.948854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.2</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.993410</td>\n",
       "      <td>0.856562</td>\n",
       "      <td>0.996282</td>\n",
       "      <td>0.948751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.5</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.994002</td>\n",
       "      <td>0.856191</td>\n",
       "      <td>0.995652</td>\n",
       "      <td>0.948615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Inter-bounded_n</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.3</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.993297</td>\n",
       "      <td>0.857213</td>\n",
       "      <td>0.997359</td>\n",
       "      <td>0.949290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Epsilon  Delta  Epoch    Metric         N         S  \\\n",
       "0     Inter-laplace    0.041    0.9     99  f1_score  0.996386  0.912798   \n",
       "3     Inter-laplace    0.021    0.6     99  f1_score  0.994994  0.884309   \n",
       "1  Inter-gaussian_a    0.010    0.8     99  f1_score  0.995502  0.889668   \n",
       "2  Inter-gaussian_a    0.041    0.4     99  f1_score  0.994643  0.891212   \n",
       "4  Inter-gaussian_a    0.031    0.9     99  f1_score  0.994041  0.870904   \n",
       "5  Inter-gaussian_a    0.010    0.4     99  f1_score  0.994804  0.870798   \n",
       "7  Inter-gaussian_a    0.021    0.4     99  f1_score  0.995754  0.865638   \n",
       "8  Inter-gaussian_a    0.041    0.2     99  f1_score  0.993410  0.856562   \n",
       "9  Inter-gaussian_a    0.021    0.5     99  f1_score  0.994002  0.856191   \n",
       "6   Inter-bounded_n    0.010    0.3     99  f1_score  0.993297  0.857213   \n",
       "\n",
       "          V  class_average  \n",
       "0  0.999063       0.969416  \n",
       "3  0.993964       0.957756  \n",
       "1  0.994406       0.959859  \n",
       "2  0.993492       0.959783  \n",
       "4  0.997514       0.954153  \n",
       "5  0.987884       0.951162  \n",
       "7  0.985169       0.948854  \n",
       "8  0.996282       0.948751  \n",
       "9  0.995652       0.948615  \n",
       "6  0.997359       0.949290  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Epsilon",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Delta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Epoch",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Metric",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "N",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "class_average",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0dd866b2-84d7-4fba-9595-a698d5c435a2",
       "rows": [
        [
         "1",
         "Inter-gaussian_a",
         "0.01",
         "0.8",
         "99",
         "f1_score",
         "0.99550176",
         "0.8896682",
         "0.9944065",
         "0.95985883"
        ],
        [
         "2",
         "Inter-gaussian_a",
         "0.041",
         "0.4",
         "99",
         "f1_score",
         "0.9946434",
         "0.8912123",
         "0.9934924",
         "0.95978266"
        ],
        [
         "4",
         "Inter-gaussian_a",
         "0.031",
         "0.9",
         "99",
         "f1_score",
         "0.99404114",
         "0.8709036",
         "0.99751395",
         "0.9541529"
        ],
        [
         "5",
         "Inter-gaussian_a",
         "0.01",
         "0.4",
         "99",
         "f1_score",
         "0.9948045",
         "0.87079835",
         "0.98788446",
         "0.9511624"
        ],
        [
         "7",
         "Inter-gaussian_a",
         "0.021",
         "0.4",
         "99",
         "f1_score",
         "0.9957545",
         "0.86563754",
         "0.9851689",
         "0.9488537"
        ],
        [
         "8",
         "Inter-gaussian_a",
         "0.041",
         "0.2",
         "99",
         "f1_score",
         "0.99341005",
         "0.85656154",
         "0.9962825",
         "0.9487514"
        ],
        [
         "9",
         "Inter-gaussian_a",
         "0.021",
         "0.5",
         "99",
         "f1_score",
         "0.9940017",
         "0.85619104",
         "0.99565214",
         "0.948615"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Metric</th>\n",
       "      <th>N</th>\n",
       "      <th>S</th>\n",
       "      <th>V</th>\n",
       "      <th>class_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.995502</td>\n",
       "      <td>0.889668</td>\n",
       "      <td>0.994406</td>\n",
       "      <td>0.959859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.4</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.994643</td>\n",
       "      <td>0.891212</td>\n",
       "      <td>0.993492</td>\n",
       "      <td>0.959783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.9</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.994041</td>\n",
       "      <td>0.870904</td>\n",
       "      <td>0.997514</td>\n",
       "      <td>0.954153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.4</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.994804</td>\n",
       "      <td>0.870798</td>\n",
       "      <td>0.987884</td>\n",
       "      <td>0.951162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.4</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.995754</td>\n",
       "      <td>0.865638</td>\n",
       "      <td>0.985169</td>\n",
       "      <td>0.948854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.2</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.993410</td>\n",
       "      <td>0.856562</td>\n",
       "      <td>0.996282</td>\n",
       "      <td>0.948751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Inter-gaussian_a</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.5</td>\n",
       "      <td>99</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.994002</td>\n",
       "      <td>0.856191</td>\n",
       "      <td>0.995652</td>\n",
       "      <td>0.948615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Epsilon  Delta  Epoch    Metric         N         S  \\\n",
       "1  Inter-gaussian_a    0.010    0.8     99  f1_score  0.995502  0.889668   \n",
       "2  Inter-gaussian_a    0.041    0.4     99  f1_score  0.994643  0.891212   \n",
       "4  Inter-gaussian_a    0.031    0.9     99  f1_score  0.994041  0.870904   \n",
       "5  Inter-gaussian_a    0.010    0.4     99  f1_score  0.994804  0.870798   \n",
       "7  Inter-gaussian_a    0.021    0.4     99  f1_score  0.995754  0.865638   \n",
       "8  Inter-gaussian_a    0.041    0.2     99  f1_score  0.993410  0.856562   \n",
       "9  Inter-gaussian_a    0.021    0.5     99  f1_score  0.994002  0.856191   \n",
       "\n",
       "          V  class_average  \n",
       "1  0.994406       0.959859  \n",
       "2  0.993492       0.959783  \n",
       "4  0.997514       0.954153  \n",
       "5  0.987884       0.951162  \n",
       "7  0.985169       0.948854  \n",
       "8  0.996282       0.948751  \n",
       "9  0.995652       0.948615  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inter_top_10.sort_values(by=[\"Model\"], ascending=False, inplace=True)\n",
    "method = \"gaussian_a\"\n",
    "laplace = inter_top_10[inter_top_10[\"Model\"] == f\"Inter-{method}\"]\n",
    "laplace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_results_loss(inter_top_10):\n",
    "\n",
    "    all_results = {}\n",
    "    loss_track = {}\n",
    "\n",
    "    # Initialize the structure\n",
    "    for index, row in inter_top_10.iterrows():\n",
    "        model = row[\"Model\"]\n",
    "        method = model.replace(\"Inter-\", \"\")\n",
    "        epsilon = row[\"Epsilon\"]\n",
    "        delta = row[\"Delta\"]\n",
    "\n",
    "        # Initialize the method if not already present\n",
    "        if method not in all_results:\n",
    "            all_results[method] = {}\n",
    "            loss_track[method] = {}\n",
    "\n",
    "        # Initialize the epsilon if not already present\n",
    "        if epsilon not in all_results[method]:\n",
    "            all_results[method][epsilon] = {}\n",
    "            loss_track[method][epsilon] = {}\n",
    "\n",
    "        # Initialize the delta if not already present\n",
    "        if delta not in all_results[method][epsilon]:\n",
    "            all_results[method][epsilon][delta] = None\n",
    "            loss_track[method][epsilon][delta] = None\n",
    "\n",
    "    return all_results, loss_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def run_program(args):\n",
    "    \n",
    "    # # Arguments\n",
    "    # logger.info(args)\n",
    "    # max_time = args.max_time # 5 3 second best 10# 40 # 100\n",
    "    # epochs = args.epochs # 300\n",
    "    # batch_size = args.batch_size # 10\n",
    "    # num_units = args.num_units\n",
    "    # bidirectional = args.bidirectional\n",
    "    # # lstm_layers = args.lstm_layers\n",
    "    # n_oversampling = args.n_oversampling\n",
    "    # checkpoint_dir = args.checkpoint_dir\n",
    "    # ckpt_name = args.ckpt_name\n",
    "    # test_steps = args.test_steps\n",
    "    # classes= args.classes\n",
    "    # filename = args.data_dir\n",
    "    # n_channels = 10\n",
    "\n",
    "\n",
    "\n",
    "all_results, loss_track = initialize_results_loss(inter_top_10)\n",
    "all_results = {'laplace': None, 'bounded_normal': None, 'gaussian_a': None}\n",
    "loss_track  = {'laplace': None, 'bounded_normal': None, 'gaussian_a': None}\n",
    "    \n",
    "for index, row in inter_top_10.iterrows():\n",
    "    model = row[\"Model\"]\n",
    "    mechanism = model.replace(\"Inter-\", \"\")\n",
    "    epsilon = row[\"Epsilon\"]\n",
    "    delta = row[\"Delta\"]\n",
    "\n",
    "    logger.info(f\"Processing entry {index} with mechanism: {mechanism}, epsilon: {epsilon}, delta: {delta}\")\n",
    "\n",
    "    # Load DP signals (all epsilon are under 0.091, hence file epsilon won't change)\n",
    "    logger.info(f\"Loading DP signals file for {mechanism} until epsilon {file_epsilon} ...\")\n",
    "    dp_signals = read_dp_signals(mechanism, file_epsilon)\n",
    "\n",
    "    logger.info(f\"Calculating data for epsilon {epsilon} and delta {delta} ...\")\n",
    "    # specifying a new directory to save the checkpoint per epsilon and delta combination \n",
    "    checkpoint_dir_temp = f\"{checkpoint_dir}_{mechanism}_e{epsilon}_d{delta}\"\n",
    "\n",
    "    # STEP 1 Read data\n",
    "    X_train, y_train = read_data(filename, dp_signals[epsilon][delta], max_time, classes=classes, max_label=50000,trainset=1)\n",
    "    X_test, y_test = read_data(filename, dp_signals[epsilon][delta], max_time, classes=classes, max_label=50000,trainset=0)          \n",
    "\n",
    "    input_depth = X_train.shape[2]\n",
    "    classes = np.unique(y_train)\n",
    "    char2numY = dict(list(zip(classes, list(range(len(classes))))))\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    for cl in classes:\n",
    "        ind = np.where(classes == cl)[0][0]\n",
    "        # logger.info(f\"Class: {cl} - count: {len(np.where(Y.flatten() == cl)[0])}\")\n",
    "\n",
    "    char2numY['<GO>'] = len(char2numY)\n",
    "    num2charY = dict(list(zip(list(char2numY.values()), list(char2numY.keys()))))\n",
    "\n",
    "    y_train = [[char2numY['<GO>']] + [char2numY[y_] for y_ in date] for date in y_train]\n",
    "    y_test  = [[char2numY['<GO>']] + [char2numY[y_] for y_ in date] for date in y_test]\n",
    "    y_test  = np.asarray(y_test)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    x_seq_length = len(X_train[0])\n",
    "    y_seq_length = len(y_train[0])- 1\n",
    "\n",
    "    # Placeholders\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    inputs = tf.compat.v1.placeholder(tf.float32, [None, max_time, input_depth], name = 'inputs')\n",
    "    targets = tf.compat.v1.placeholder(tf.int32, (None, None), 'targets')\n",
    "    dec_inputs = tf.compat.v1.placeholder(tf.int32, (None, None), 'output')\n",
    "\n",
    "    logits = build_network(inputs, dec_inputs, char2numY, n_channels=n_channels, input_depth=input_depth, num_units=num_units, max_time=max_time,\n",
    "                bidirectional=bidirectional)\n",
    "    \n",
    "    with tf.compat.v1.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        vars = tf.compat.v1.trainable_variables()\n",
    "        beta = 0.001\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if 'bias' not in v.name]) * beta\n",
    "        loss = tfa.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, y_seq_length]))\n",
    "        loss = tf.reduce_mean(input_tensor=loss + lossL2)\n",
    "        # Optimizer\n",
    "        optimizer = tf.compat.v1.train.RMSPropOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "    # over-sampling: SMOTE\n",
    "    X_train = np.reshape(X_train,[X_train.shape[0]*X_train.shape[1],-1])\n",
    "    y_train= y_train[:,1:].flatten()\n",
    "\n",
    "    nums = []\n",
    "    for cl in classes:\n",
    "        ind = np.where(classes == cl)[0][0]\n",
    "        nums.append(len(np.where(y_train.flatten()==ind)[0]))\n",
    "\n",
    "    ratio={0:nums[0],1:n_oversampling+1000,2:n_oversampling*2}\n",
    "    sm = SMOTE(random_state=12,sampling_strategy=ratio)\n",
    "\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    X_train = X_train[:int(X_train.shape[0]/max_time)*max_time,:]\n",
    "    y_train = y_train[:int(X_train.shape[0]/max_time)*max_time]\n",
    "\n",
    "    X_train = np.reshape(X_train,[-1,X_test.shape[1],X_test.shape[2]])\n",
    "    y_train = np.reshape(y_train,[-1,y_test.shape[1]-1,])\n",
    "\n",
    "    y_train= [[char2numY['<GO>']] + [y_ for y_ in date] for date in y_train]\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    if (os.path.exists(checkpoint_dir_temp) == False):\n",
    "        os.mkdir(checkpoint_dir_temp)\n",
    "    \n",
    "    epoch_var = tf.Variable(0, dtype=tf.int32, trainable=False, name=\"epoch\")\n",
    "    \n",
    "    # train the graph\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        sess.run(tf.compat.v1.local_variables_initializer())\n",
    "\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        # saver = tf.compat.v1.train.Saver(var_list=tf.compat.v1.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir_temp)\n",
    "\n",
    "        epoch_loss = {}\n",
    "        epoch_results = {}\n",
    "        \n",
    "        for epoch_i in range(epochs):\n",
    "            test_results = {}\n",
    "            start_time = time.time()\n",
    "            train_acc = []\n",
    "            batches_loss = {}\n",
    "            for batch_i, (source_batch, target_batch) in enumerate(batch_data(X_train, y_train, batch_size)):\n",
    "                _, batch_loss, batch_logits = sess.run([optimizer, loss, logits],\n",
    "                    feed_dict = {inputs: source_batch,\n",
    "                                    dec_inputs: target_batch[:, :-1],\n",
    "                                    targets: target_batch[:, 1:]})\n",
    "                batches_loss[batch_i] = batch_loss\n",
    "                train_acc.append(batch_logits.argmax(axis=-1) == target_batch[:,1:])\n",
    "            \n",
    "            epoch_loss[epoch_i] = batches_loss\n",
    "\n",
    "            accuracy = np.mean(train_acc)\n",
    "            logger.info(f\"Epoch {epoch_i+1} Loss: {batch_loss} Accuracy: {accuracy} Epoch duration: {time.time() - start_time}s \")\n",
    "\n",
    "            if (epoch_i+1)%test_steps==0:\n",
    "                acc_avg, acc, sensitivity, specificity, ppv, npv, fpr, fnr, fdr, f1_score = test_model(sess, logits, X_test, y_test, batch_size, char2numY, y_seq_length, inputs, dec_inputs) # definition moved to function definitions (Ricarda)\n",
    "                test_results[\"avg_acc\"] = acc_avg\n",
    "                test_results[\"acc\"]     = acc\n",
    "                test_results[\"sens\"]    = sensitivity\n",
    "                test_results[\"spec\"]    = specificity\n",
    "                test_results[\"prec\"]    = ppv\n",
    "                test_results[\"neg_pred_value\"] = npv\n",
    "                test_results[\"false_pos_rate\"] = fpr\n",
    "                test_results[\"false_neg_rate\"] = fnr\n",
    "                test_results[\"false_det_rate\"] = fdr\n",
    "                test_results[\"f1_score\"]       = f1_score\n",
    "                # test_results[\"class_results\"]  = class_results\n",
    "\n",
    "                epoch_results[epoch_i] = test_results\n",
    "            \n",
    "            save_path = os.path.join(checkpoint_dir_temp, ckpt_name)\n",
    "            sess.run(tf.compat.v1.assign(epoch_var, epoch_i+1))\n",
    "            saver.save(sess, save_path)\n",
    "            logger.info(f\"Model saved in path {save_path}\")\n",
    "            \n",
    "        all_results[mechanism][epsilon][delta] = epoch_results\n",
    "        loss_track[mechanism][epsilon][delta] = epoch_loss\n",
    "\n",
    "with open(osj(\"..\", \"results_shadow\", \"interpatient_results.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(all_results, f) \n",
    "with open(osj(\"..\", \"results_shadow\", \"interpatient_loss.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(loss_track, f) \n",
    "\n",
    "logger.info(f\"Saved results and loss for all combinations.\")\n",
    "logger.info(f\"Model training finished!\")\n",
    "\n",
    "\n",
    "# # 6 configure arguments\n",
    "# def main():\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--epochs', type=int, default=100)\n",
    "#     parser.add_argument('--max_time', type=int, default=20) # originally 10\n",
    "#     parser.add_argument('--test_steps', type=int, default=10)\n",
    "#     parser.add_argument('--batch_size', type=int, default=10) # originally 20\n",
    "#     parser.add_argument('--bidirectional', type=str2bool, default=str2bool('True')) # originally False\n",
    "#     # parser.add_argument('--lstm_layers', type=int, default=2)\n",
    "#     parser.add_argument('--num_units', type=int, default=128)\n",
    "#     parser.add_argument('--n_oversampling', type=int, default=10000)\n",
    "#     parser.add_argument('--data_dir', type=str, default='../data/s2s_mitbih_aami')\n",
    "#     parser.add_argument('--checkpoint_dir', type=str, default='dp_checkpoints_DS/shadow/checkpoints')\n",
    "#     parser.add_argument('--ckpt_name', type=str, default='seq2seq.ckpt')\n",
    "#     parser.add_argument('--classes', nargs='+', type=chr, default=['N','S','V']) \n",
    "#     args = parser.parse_args()\n",
    "#     run_program(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
